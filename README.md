# FabricLabs


# Agenda for the Day
**08:00-09:00:** Arrival, registration and breakfast  
**09:00-09:30:** Intro to Fabric and the use cases for the platform w/ Brian & Mathias  
**09:30-10:20:** Ingestion of data using Data Pipelines - with hands on labs w/ Mathias  
**10:20-11:10:** Data transformation using Notebooks and Dataflow gen 2 - with hands on labs - w/ Henri & Christian  
**11:10-11:20:** 10 min break  
**11:20-12:10:** Data Warehouse in Fabric - with hands on labs w/ Christian  
**12:10-13:00:** Lunch  
**13:00-13:50:** Machine learning in Fabric - with hands on labs w/ Henri  
**13:50-14:00:** 10 min break  
**14:00-14:50:** Realtime analytics - with hands on labs w/ Brian  
**14:50-15:00:** 10 min break  
**15:00-15:50:** Power BI analytics using Direkt Lake - with hands on labs w/ Lars  
**15:50-16:30:** Pespectives, Roll-off and closing  




# Datawarehousing

In this lab we will go through the "Data Warehouse" experience from the [Multi-experience Tutorials](https://learn.microsoft.com/en-us/fabric/data-warehouse/tutorial-introduction). This is intended to showcase the capabilities of Data Warehousing inside of Microsoft Fabric.

First we will however start with a presentation of the capabilities of Fabric data warehousing, how it differs from other Microsoft data warehousing, and when to use it. 

# Real-Time Analytics

This lab will focus on the Kusto Experience in Fabric and use the [Real-time tutorial from Microsoft online](https://learn.microsoft.com/en-us/fabric/real-time-analytics/tutorial-introduction). 
This lab is intended to demo the different possibilities with Kusto inside the unified experience of Fabric.

# Direct Lake

The content for this lab is located [here](https://github.com/CHMAND/FabricLabs/blob/main/Lab%20-%20Build%20a%20Direct%20Lake%20dataset.pdf). It is inspired by [Lakehouse end-to-end scenario: overview and architecture](https://learn.microsoft.com/en-us/fabric/data-engineering/tutorial-lakehouse-introduction). This lab is more basic and focus on creating a simple lakehouse, simple ingestion of sample data, a Direct Lake dataset and creating a report.
